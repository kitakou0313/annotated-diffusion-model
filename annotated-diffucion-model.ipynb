{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diffusion-modelとは？\n",
    "\n",
    "- 単純な分布から得たノイズをデータサンプルに変換するモデル\n",
    "- 純粋なノイズからNNが徐々にノイズ除去の方法を学習していく\n",
    "\n",
    "以下の二つのプロセスから構成される\n",
    "\n",
    "### Forward diffusion process\n",
    "- データにガウス分布から生成したノイズを加算していき、十分純粋なノイズとなるまでTステップ繰り返す\n",
    "    - DDPMでは1000くらい\n",
    "    - 十分Tが大きく、また「a well behaved schedule for adding noise at each time step」であればisotropic Gaussian distributionと見なせるらしい\n",
    "        - https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic\n",
    "\n",
    "### Learned reverse denoising diffusion process\n",
    "- NNが純粋なノイズから元の画像になるまでノイズ除去できるよう学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN実装用のヘルパー関数\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "\n",
    "def Upsample(dim):\n",
    "    \"\"\"\n",
    "    アップサンプル用の関数\n",
    "    \"\"\"\n",
    "    return nn.ConvTranspose2d(dim, dim, 4,2,1)\n",
    "\n",
    "def Downsample(dim):\n",
    "    \"\"\"\n",
    "    ダウンサンプリング\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(dim, dim, 4,2,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tの埋め込み\n",
    "\n",
    "- transformerと同じくPositional Embeddingsを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tのpositionalEmbeddingクラス\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        dimは埋め込み先次元\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, time:torch.Tensor):\n",
    "        \"\"\"\n",
    "        tの埋め込み\n",
    "        \"\"\"\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        embeddings = math.log(10000) / (half_dim-1)\n",
    "        embeddings = torch.exp(torch.arange(\n",
    "            half_dim, device=device\n",
    "        ) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "    def __init__(self,dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, c, scale_shift=None):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x*(scale + 1) + shift\n",
    "\n",
    "        \n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(\n",
    "            dim, dim_out, 1\n",
    "        ) if dim != dim_out else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, time_emb=None):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        h = self.block1(x)\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
    "\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2201.03545\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(\n",
    "                nn.GELU(), nn.Linear(time_emb_dim, dim)\n",
    "            ) if exists(time_emb_dim) else None\n",
    "        )\n",
    "\n",
    "        self.ds_conv = nn.Conv2d(\n",
    "            dim, dim, 7, padding=3, groups=dim\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out*mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, dim_out*mult),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(\n",
    "            dim, dim_out, 1\n",
    "        ) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            condition = self.mlp(time_emb)\n",
    "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
    "        \n",
    "        h = self.net(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        b,c ,h,w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3,dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(\n",
    "                t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv2d(\n",
    "            dim, hidden_dim*3, 1, bias=False\n",
    "        )\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),nn.GroupNorm(1, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        b,c,h,w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(\n",
    "                t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\",\n",
    "                        h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import group\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1,2,4,8),\n",
    "        channels=3,\n",
    "        with_time_emb=True,\n",
    "        resnet_block_groups=8,\n",
    "        use_convnext=True,\n",
    "        convnext_mult=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "\n",
    "        dims = [\n",
    "            init_dim, *map(lambda m: dim*m, dim_mults)\n",
    "        ]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        if use_convnext:\n",
    "            block_klass = partial(\n",
    "                ConvNextBlock, mult=convnext_mult\n",
    "            )\n",
    "        else:\n",
    "            block_klass = partial(\n",
    "                ResNetBlock, groups=resnet_block_groups\n",
    "            )\n",
    "\n",
    "        if with_time_emb:\n",
    "            time_dim = dim*4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim)\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            mid_dim = dims[-1]\n",
    "            self.mid_block1 = block_klass(\n",
    "                mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "            self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "            self.mid_block2 = block_klass(\n",
    "                mid_dim, mid_dim, time_emb_dim=time_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
